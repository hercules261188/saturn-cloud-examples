---
title: "1 GPU vs 3 GPU speed comparison"
output: html_document
---

This example shows the difference between training a neural network on a single GPU vs 3 GPUs on Saturn Cloud. The data comes from the text-generation-nn example (but specifically the comparison notebook in this analysis subfolder). The neural network is relatively small (four LSTM layers with 128 nodes), and the data is only tens of thousands of pet names, so not that large.

The data shows that on a per-epoch basis, the 3-GPU model is equivalent to the 1-GPU model with 3-times the batch size. This isn't surprising since at the end of each batch all the 3-GPU results are compiled together into a single batch to compute the new values.

That said, simple models generally learn faster from smaller batch sizes, so it takes fewer epochs for a smaller batch size model to train. Here we can see that the model does best with a small batch and high learning rate. Further, from the time based graph, we can see that the cost of communicating between the GPUs is sufficiently high that each epoch trains quite a bit slower in the multi-GPU case. Putting this all together, the best configuration for this model is a single GPU with low batch size and high learning rate. This shows that not all models benefit from multi-GPU setups.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, dpi = 300, fig.width = 8, fig.height = 6)
```

```{r libararies,  messages=FALSE}
library(tidyverse)
library(fs)
library(jsonlite)
library(glue)
```

```{r load_data}
data <- 
  dir_ls("training-comparisons/2021-02-18T20_05_31.301849", recurse=TRUE, regexp = ".json$") %>%
  str_match("^training-comparisons/2021-02-18T20_05_31.301849/batch=([0-9]+)\\&num_workers=([0-9]+)\\&learning_rate_multiplier=([0-9\\.]+)/logs/data_([0-9])+_.*") %>%
  as_tibble(.name_repair = ~ c("path", "batch", "num_workers", "learning_rate_multiplier", "worker")) %>%
  mutate_at(vars(batch, num_workers, worker,learning_rate_multiplier), as.numeric) %>%
  mutate(data = map(path, read_json)) %>%
  select(-path, -worker) %>%
  unnest_wider(data) %>%
  ungroup()
```


```{r plot_by_epoch}
data %>%
  filter(worker == 0) %>%
  mutate(group = glue("workers={num_workers} batch={batch} learning-multi={learning_rate_multiplier}")) %>%
  ggplot(aes(x=epoch, y = loss, group = group, color = group, shape = factor(num_workers))) +
  geom_point() +
  geom_line()
```

```{r plot_by_time}
data %>%
  filter(worker == 0) %>%
  mutate(group = glue("workers={num_workers} batch={batch} learning-multi={learning_rate_multiplier}")) %>%
  ggplot(aes(x=elapsed_time, y = loss, group = group, color = group, shape = factor(num_workers))) +
    geom_point() +
    geom_line()
```
